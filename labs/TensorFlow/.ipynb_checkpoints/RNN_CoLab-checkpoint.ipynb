{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-BvtY73iFaj8"
   },
   "source": [
    "# Tensor Flow on CoLabotory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1DKtEOv6Ho5W"
   },
   "source": [
    "### Step 1: Getting  Input Files from AWS Bucket\n",
    "\n",
    "Public S3 data can be easily accessed by using `!wget` in CoLabotory; however, the S3 bucket of this Lab and Assignment is private.  In this case, we need to upload input files into CoLabotory directory: \n",
    "\n",
    "1) Please download input data files from Canvas, and save them into your local directory.  \n",
    "\n",
    "2) Go to files tab:  \n",
    "\n",
    "![How to bring up files tab](https://raw.githubusercontent.com/rbm2/RiceCOMP543/master/Colab_00.png)\n",
    "\n",
    "3) Click `UPLOAD`, and select input data files on your local drive. \n",
    "\n",
    "4) Once files are uploaded, you should be able to see them in the \"Files\" tab as following:   \n",
    "\n",
    "![Uploaded Files](https://raw.githubusercontent.com/rbm2/RiceCOMP543/master/Colab_01.png)\n",
    "\n",
    "5) Please notice that those files will be deleted when runtime is reset. So you may need to repeat this step after you selected `Reset All Runtimes...` (`Restart runtime` will keep your files. )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BI_DNw0rMnBu"
   },
   "source": [
    "### Step 2: Training Visualization Using TensorBoard\n",
    "\n",
    "Tensorboard is a great tool to visualize your trainning process, and you can find some example [here](https://www.tensorflow.org/guide/summaries_and_tensorboard) to help you complete this lab. In this lab, we'll use a free service called [ngrok](https://ngrok.com/) to build the connection between Google server and you local machine. Please follow steps below to setup your Tensorboard connection: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fmYzsy_FKxQ6"
   },
   "source": [
    "#### a) Download and unzip ngrok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "YNZryMUIMgzn"
   },
   "outputs": [],
   "source": [
    "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
    "!unzip ngrok-stable-linux-amd64.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JT0IDh7hLBOo"
   },
   "source": [
    "#### b) Run TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "_UW63VcbLBiw"
   },
   "outputs": [],
   "source": [
    "LOG_DIR = './log'\n",
    "get_ipython().system_raw(\n",
    "    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n",
    "    .format(LOG_DIR)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mC46nkmTLKYp"
   },
   "source": [
    "#### c) Run ngrok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "3uOMCKEMLPNT"
   },
   "outputs": [],
   "source": [
    "get_ipython().system_raw('./ngrok http 6006 &')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LyR-NBSOLShp"
   },
   "source": [
    "#### d) Get URL\n",
    "\n",
    "Generate URL for TensorBoard\n",
    " - You only need to generate this link ONCE - you can keep using this url address for your TensorBoard until your runtime is terminated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "GEJJ6CX7Lg1K"
   },
   "outputs": [],
   "source": [
    "! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
    "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BxuFR6EsNeEG"
   },
   "source": [
    "### Step 3: Enable GPU accelerator to speed up your training:\n",
    "\n",
    "Now, running deep learning on different hardwares can make huge difference on runtime. Fortunately, CoLabotory offers GPU accelerator to speed up your training process for FREE. Before running your code, please make sure to enable the GPU accelerator  as following: \n",
    "\n",
    "1)  Go to Menu bar, select `Edit`  \n",
    "2)  Select `Notebook Settings`  \n",
    "3)  Enable GPU Accelerator, and SAVE\n",
    "![GPU Accelerator](https://raw.githubusercontent.com/rbm2/RiceCOMP543/master/Colab_02.png)  \n",
    "4)  Now you are good to run your Tensorflow code.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l58AzlcnI8zL"
   },
   "source": [
    "### Step 4: Run your Lab 7 Code on CoLab\n",
    "- Please follow steps below to complete this task:\n",
    "\n",
    "    1) Make the same modification as the AWS part to display the message at the end of training.\n",
    "    \n",
    "    2) Uncomment the Tensorboard code in the code (line 207-226, 264-279) below. Now, you can simply test your code and take a quick look of TensorBoard. **Hint**: To test your code, you can reduce `numTrainingIters` to a smaller number. \n",
    "    \n",
    "    3) In TensorBoard setup code (line 207-226), please refer to Tensorboard example ([link](https://www.tensorflow.org/guide/summaries_and_tensorboard)), and plot either of the following: `historgram of prediction2` or `mean of Weight in hidden layer`. Of course, feel free to try more. **Hint**: You can enable line number display in `Tools` -> `Preference...` -> `Show Line Number`\n",
    "    \n",
    "    3) To check off, please run 5000 iterations, show last 20 output (to avoid slowing browser down, we will print output every 100 iterations in CoLab), the message in the end, and plots in Tensorboard to a TA/ Instructor. \n",
    "\n",
    "- To avoid Error Message,  please always go to `Runtime` in the menu bar, click `Restart Runtime` before you rerun the training. \n",
    "\n",
    "- To visualize your training progress,  please make sure that you run the code in **Step 2**, complete the Tensorflow code,  and click the Tensorboard link generated in step d) after training is done.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "U3tpi5Olb6Yp"
   },
   "outputs": [],
   "source": [
    "# You may need to clean up the log file \n",
    "# to avoid overlap with your old graph from previous run. \n",
    "!rm log -rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "miejOdncEyHi"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "# the number of iterations to train for\n",
    "numTrainingIters = 5000\n",
    "\n",
    "# the number of hidden neurons that hold the state of the RNN\n",
    "hiddenUnits = 1000\n",
    "\n",
    "# the number of classes that we are learning over\n",
    "numClasses = 3\n",
    "\n",
    "# the number of data points in a batch\n",
    "batchSize = 100\n",
    "\n",
    "# this function takes a dictionary (called data) which contains \n",
    "# of (dataPointID, (classNumber, matrix)) entries.  Each matrix\n",
    "# is a sequence of vectors; each vector has a one-hot-encoding of\n",
    "# an ascii character, and the sequence of vectors corresponds to\n",
    "# one line of text.  classNumber indicates which file the line of\n",
    "# text came from.  \n",
    "# \n",
    "# The argument maxSeqLen is the maximum length of a line of text\n",
    "# seen so far.  fileName is the name of a file whose contents\n",
    "# we want to add to data.  classNum is an indicator of the class\n",
    "# we are going to associate with text from that file.  linesToUse\n",
    "# tells us how many lines to sample from the file.\n",
    "#\n",
    "# The return val is the new maxSeqLen, as well as the new data\n",
    "# dictionary with the additional lines of text added\n",
    "def addToData (maxSeqLen, data, fileName, classNum, linesToUse):\n",
    "    #\n",
    "    # open the file and read it in\n",
    "    with open(fileName) as f:\n",
    "        content = f.readlines()\n",
    "    #\n",
    "    # sample linesToUse numbers; these will tell us what lines\n",
    "    # from the text file we will use\n",
    "    # [Note] random_integers genetate a vector with size \"linesToUse\", rand from 0 to len(content)\n",
    "    myInts = np.random.random_integers (0, len(content) - 1, linesToUse)\n",
    "    #\n",
    "    # i is the key of the next line of text to add to the dictionary\n",
    "    # [Note] dictionary is called \"data\" in this case, so i is the length of dictionary\n",
    "    i = len(data)\n",
    "    #\n",
    "    # loop thru and add the lines of text to the dictionary\n",
    "    for whichLine in myInts.flat: # myInts.flat is a 1-D interator over myInts\n",
    "        #\n",
    "        # get the line and ignore it if it has nothing in it\n",
    "        line = content[whichLine]\n",
    "        if line.isspace () or len(line) == 0:\n",
    "            continue;\n",
    "        #\n",
    "        # take note if this is the longest line we've seen\n",
    "        if len (line) > maxSeqLen:\n",
    "            maxSeqLen = len (line)\n",
    "        #\n",
    "        # create the matrix that will hold this line\n",
    "        temp = np.zeros((len(line), 256))\n",
    "        #\n",
    "        # j is the character we are on\n",
    "        j = 0\n",
    "        # \n",
    "        # loop thru the characters\n",
    "        for ch in line:\n",
    "            #\n",
    "            # non-ascii? ignore\n",
    "            if ord(ch) >= 256: # ord(c) gives the unicode of c\n",
    "                continue\n",
    "            #\n",
    "            # one hot!\n",
    "            temp[j][ord(ch)] = 1 # mark the ascii index \n",
    "            # \n",
    "            # move onto the next character\n",
    "            j = j + 1\n",
    "            #\n",
    "        # remember the line of text\n",
    "        # add this (class number, matrix_of_line) to end of data (dictionary)\n",
    "        data[i] = (classNum, temp)\n",
    "        #\n",
    "        # move onto the next line\n",
    "        i = i + 1\n",
    "    #\n",
    "    # and return the dictionary with the new data\n",
    "    return (maxSeqLen, data) # (max length of the line in file, and the dictionary)\n",
    "\n",
    "# this function takes as input a data set encoded as a dictionary\n",
    "# (same encoding as the last function) and pre-pends every line of\n",
    "# text with empty characters so that each line of text is exactly\n",
    "# maxSeqLen characters in size\n",
    "def pad (maxSeqLen, data):\n",
    "   #\n",
    "   # loop thru every line of text\n",
    "   for i in data:\n",
    "        #\n",
    "        # access the matrix and the label\n",
    "        temp = data[i][1]\n",
    "        label = data[i][0]\n",
    "        # \n",
    "        # get the number of chatacters in this line\n",
    "        len = temp.shape[0]\n",
    "        #\n",
    "        # and then pad so the line is the correct length\n",
    "        padding = np.zeros ((maxSeqLen - len,256)) \n",
    "        data[i] = (label, np.transpose (np.concatenate ((padding, temp), axis = 0)))\n",
    "   #\n",
    "   # return the new data set\n",
    "   return data\n",
    "\n",
    "# this generates a new batch of training data of size batchSize from the\n",
    "# list of lines of text data. This version of generateData is useful for\n",
    "# an RNN because the data set x is a NumPy array with dimensions\n",
    "# [batchSize, 256, maxSeqLen]; it can be unstacked into a series of\n",
    "# matrices containing one-hot character encodings for each data point\n",
    "# using tf.unstack(inputX, axis=2)\n",
    "def generateDataRNN (maxSeqLen, data):\n",
    "    #\n",
    "    # randomly sample batchSize lines of text\n",
    "    myInts = np.random.random_integers (0, len(data) - 1, batchSize)\n",
    "    #\n",
    "    # stack all of the text into a matrix of one-hot characters\n",
    "    x = np.stack (data[i][1] for i in myInts.flat)\n",
    "    #\n",
    "    # and stack all of the labels into a vector of labels\n",
    "    y = np.stack (np.array((data[i][0])) for i in myInts.flat)\n",
    "    #\n",
    "    # return the pair\n",
    "    return (x, y)\n",
    "\n",
    "# this also generates a new batch of training data, but it represents\n",
    "# the data as a NumPy array with dimensions [batchSize, 256 * maxSeqLen]\n",
    "# where for each data point, all characters have been appended.  Useful\n",
    "# for feed-forward network training\n",
    "def generateDataFeedForward (maxSeqLen, data):\n",
    "    #\n",
    "    # randomly sample batchSize lines of text\n",
    "    myInts = np.random.random_integers (0, len(data) - 1, batchSize)\n",
    "    #\n",
    "    # stack all of the text into a matrix of one-hot characters\n",
    "    x = np.stack (data[i][1].flatten () for i in myInts.flat) # flatten turns matrix into 1-D form\n",
    "    #\n",
    "    # and stack all of the labels into a vector of labels\n",
    "    y = np.stack (np.array((data[i][0])) for i in myInts.flat)\n",
    "    #\n",
    "    # return the pair\n",
    "    return (x, y)\n",
    "\n",
    "# create the data dictionary\n",
    "maxSeqLen = 0\n",
    "data = {}\n",
    "\n",
    "# load up the three data sets\n",
    "(maxSeqLen, data) = addToData (maxSeqLen, data, \"Holmes.txt\", 0, 10000)\n",
    "(maxSeqLen, data) = addToData (maxSeqLen, data, \"war.txt\", 1, 10000)\n",
    "(maxSeqLen, data) = addToData (maxSeqLen, data, \"william.txt\", 2, 10000)\n",
    "\n",
    "# pad each entry in the dictionary with empty characters as needed so\n",
    "# that the sequences are all of the same length\n",
    "data = pad (maxSeqLen, data)\n",
    "        \n",
    "# now we build the TensorFlow computation... there are two inputs, \n",
    "# a batch of text lines and a batch of labels\n",
    "inputX = tf.placeholder(tf.float32, [batchSize, 256, maxSeqLen])\n",
    "inputY = tf.placeholder(tf.int32, [batchSize])\n",
    "\n",
    "# this is the inital state of the RNN, before processing any data\n",
    "initialState = tf.placeholder(tf.float32, [batchSize, hiddenUnits])\n",
    "\n",
    "# the weight matrix that maps the inputs and hidden state to a set of values\n",
    "W = tf.Variable(np.random.normal(0, 0.05, (hiddenUnits + 256, hiddenUnits)), dtype=tf.float32)\n",
    "\n",
    "# biaes for the hidden values\n",
    "b = tf.Variable(np.zeros((1, hiddenUnits)), dtype=tf.float32)\n",
    "\n",
    "# weights and bias for the final classification\n",
    "W2 = tf.Variable(np.random.normal (0, 0.05, (hiddenUnits, numClasses)),dtype=tf.float32)\n",
    "b2 = tf.Variable(np.zeros((1,numClasses)), dtype=tf.float32)\n",
    "\n",
    "# unpack the input sequences so that we have a series of matrices,\n",
    "# each of which has a one-hot encoding of the current character from\n",
    "# every input sequence\n",
    "sequenceOfLetters = tf.unstack(inputX, axis=2)\n",
    "\n",
    "# now we implement the forward pass\n",
    "currentState = initialState\n",
    "for timeTick in sequenceOfLetters:\n",
    "    #\n",
    "    # concatenate the state with the input, then compute the next state\n",
    "    inputPlusState = tf.concat([timeTick, currentState], 1)  \n",
    "    next_state = tf.tanh(tf.matmul(inputPlusState, W) + b) \n",
    "    currentState = next_state\n",
    "\n",
    "# compute the set of outputs\n",
    "outputs = tf.matmul(currentState, W2) + b2 # matmul\n",
    "\n",
    "predictions = tf.nn.softmax(outputs) # softmax\n",
    "\n",
    "# compute the loss\n",
    "losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=outputs, labels=inputY)\n",
    "totalLoss = tf.reduce_mean(losses)\n",
    "\n",
    "# use gradient descent to train\n",
    "#trainingAlg = tf.train.GradientDescentOptimizer(0.02).minimize(totalLoss)\n",
    "trainingAlg = tf.train.AdagradOptimizer(0.02).minimize(totalLoss)\n",
    "\n",
    "# # TensorBoard Steup below ----------------\n",
    "# # add Loss to summary\n",
    "# tf.summary.scalar('Loss', totalLoss)\n",
    "\n",
    "# # Refer to Tensorboard example, please plot either of following: \n",
    "# # (Of course, feel free to try both!)\n",
    "# # 1 - historgram of prediction\n",
    "# # 2 - mean of Weight in hidden layer\n",
    "# # put you code here: \n",
    "\n",
    "\n",
    "# # directory where the results from the training are saved\n",
    "# result_dir = './log/' \n",
    "\n",
    "# # Build the summary operation based on the TF collection of Summaries.\n",
    "# summary_op = tf.summary.merge_all()\n",
    "\n",
    "# # Instantiate a SummaryWriter to output summaries and the Graph.\n",
    "# summary_writer = tf.summary.FileWriter(result_dir, sess.graph)\n",
    "# # Tensorboard Stepup above ---------------\n",
    "\n",
    "\n",
    "# and train!!\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    #\n",
    "    # initialize everything\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #\n",
    "    # and run the training iters\n",
    "    for epoch in range(numTrainingIters):\n",
    "        # \n",
    "        # get some data\n",
    "        x, y = generateDataRNN (maxSeqLen, data)\n",
    "        #\n",
    "        # do the training epoch\n",
    "        _currentState = np.zeros((batchSize, hiddenUnits))\n",
    "        _totalLoss, _trainingAlg, _currentState, _predictions, _outputs = sess.run(\n",
    "                [totalLoss, trainingAlg, currentState, predictions, outputs],\n",
    "                feed_dict={\n",
    "                    inputX:x,\n",
    "                    inputY:y,\n",
    "                    initialState:_currentState\n",
    "                })\n",
    "        #        \n",
    "        # just FYI, compute the number of correct predictions\n",
    "        numCorrect = 0\n",
    "        for i in range (len(y)):\n",
    "            maxPos = -1\n",
    "            maxVal = 0.0\n",
    "            for j in range (numClasses):\n",
    "                if maxVal < _predictions[i][j]:\n",
    "                    maxVal = _predictions[i][j]\n",
    "                    maxPos = j\n",
    "            if maxPos == y[i]:\n",
    "                numCorrect = numCorrect + 1\n",
    "        \n",
    "#         # Tensorboard below ----------------\n",
    "#         # output the training summary every 100 iterations\n",
    "#         if epoch % 100 == 0:\n",
    "#             # print out to the screen\n",
    "#             print(\"Step\", epoch, \"Loss\", _totalLoss, \"Correct\", numCorrect, \"out of\", batchSize)\n",
    "#             # Update the events file which is used to monitor the training.\n",
    "#             summary_str = sess.run(\n",
    "#                 summary_op,\n",
    "#                 feed_dict={\n",
    "#                     inputX:x,\n",
    "#                     inputY:y,\n",
    "#                     initialState:_currentState\n",
    "#                 })\n",
    "#             summary_writer.add_summary(summary_str, epoch)\n",
    "#             summary_writer.flush()         \n",
    "#         # Tensorboard above -----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reminder: \n",
    "Now, please refer back to the URL you have generated in step 2-d, and click that link to check your TensorBoard. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright Â©2019 Christopher M Jermaine (cmj4@rice.edu), and Risa B Myers  (rbm2@rice.edu)\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "    https://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "RNN_CoLab.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
